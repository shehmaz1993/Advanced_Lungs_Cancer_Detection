{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1079953,"sourceType":"datasetVersion","datasetId":601280},{"sourceId":2882784,"sourceType":"datasetVersion","datasetId":1748489},{"sourceId":11339277,"sourceType":"datasetVersion","datasetId":7093867}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\n# Clear any existing GPU settings\ntf.keras.backend.clear_session()\n\n# Set TensorFlow to run on GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Allow memory growth to prevent OOM (Out-of-Memory) errors\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPU is set up successfully!\")\n    except RuntimeError as e:\n        print(f\"GPU Setup Error: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:40:54.143341Z","iopub.execute_input":"2025-04-12T20:40:54.143552Z","iopub.status.idle":"2025-04-12T20:41:08.417710Z","shell.execute_reply.started":"2025-04-12T20:40:54.143526Z","shell.execute_reply":"2025-04-12T20:41:08.417000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\nfrom PIL import Image\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:41:08.418416Z","iopub.execute_input":"2025-04-12T20:41:08.418895Z","iopub.status.idle":"2025-04-12T20:41:09.141912Z","shell.execute_reply.started":"2025-04-12T20:41:08.418877Z","shell.execute_reply":"2025-04-12T20:41:09.141373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define dataset paths\ndataset1_path = \"/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets\"\ndataset2_path = \"/kaggle/input/iqothnccd-lung-cancer-dataset/The IQ-OTHNCCD lung cancer dataset/The IQ-OTHNCCD lung cancer dataset\"\ncombined_dataset_path = \"/kaggle/working/Combined_Lung_Dataset\"\n\n# Mapping folders to 3-class structure\ndatasets = {\n    \"lung_n\": \"Normal\",\n    \"Normal cases\": \"Normal\",\n    \"Bengin cases\": \"Benign\",\n    \"lung_aca\": \"Malignant\",\n    \"lung_scc\": \"Malignant\",\n    \"Malignant cases\": \"Malignant\",\n}\n\n# Make sure combined class folders exist\nfor category in set(datasets.values()):\n    os.makedirs(os.path.join(combined_dataset_path, category), exist_ok=True)\n\n# Function to copy & rename images with prefix\ndef copy_images(source_folder, target_folder, prefix):\n    if os.path.exists(source_folder):\n        for file in os.listdir(source_folder):\n            if file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n                src_path = os.path.join(source_folder, file)\n                new_filename = f\"{prefix}_{file}\"\n                dst_path = os.path.join(target_folder, new_filename)\n                shutil.copy(src_path, dst_path)\n\n# Copy dataset 1 (H&E histology) ‚Üí add \"histo_\" prefix\nfor folder, category in datasets.items():\n    source_folder = os.path.join(dataset1_path, folder)\n    target_folder = os.path.join(combined_dataset_path, category)\n    copy_images(source_folder, target_folder, prefix=\"histo\")\n\n# Copy dataset 2 (CT scans) ‚Üí add \"ct_\" prefix\nfor folder, category in datasets.items():\n    source_folder = os.path.join(dataset2_path, folder)\n    target_folder = os.path.join(combined_dataset_path, category)\n    copy_images(source_folder, target_folder, prefix=\"ct\")\n\n# Cleanup: remove any empty folders\nfor category in set(datasets.values()):\n    target_folder = os.path.join(combined_dataset_path, category)\n    if len(os.listdir(target_folder)) == 0:\n        print(f\"Removing empty folder: {target_folder}\")\n        shutil.rmtree(target_folder)\n\nprint(\"‚úÖ Dataset successfully merged into 3 classes: Normal, Benign, Malignant with histo/ct prefixes!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:41:09.143916Z","iopub.execute_input":"2025-04-12T20:41:09.144733Z","iopub.status.idle":"2025-04-12T20:43:21.530707Z","shell.execute_reply.started":"2025-04-12T20:41:09.144713Z","shell.execute_reply":"2025-04-12T20:43:21.529953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# === Paths ===\nreal_benign = \"/kaggle/working/Combined_Lung_Dataset/Benign\"\nsynthetic_benign = \"/kaggle/input/gan-images-iq-othnccd-benign\"\ntarget_non_malignant = \"/kaggle/working/Combined_Lung_Dataset/Non-Malignant\"\n\n# Create target folder\nos.makedirs(target_non_malignant, exist_ok=True)\n\n# === Copy Real Benign Images (prefix: real_) ===\nfor filename in os.listdir(real_benign):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n        src = os.path.join(real_benign, filename)\n        dst = os.path.join(target_non_malignant, f\"real_{filename}\")\n        shutil.copyfile(src, dst)\n\n# === Copy Synthetic Benign Images (prefix: ct_synthetic_) ===\nfor filename in os.listdir(synthetic_benign):\n    if filename.lower().endswith('.png'):  # All synthetic are .png\n        src = os.path.join(synthetic_benign, filename)\n        dst = os.path.join(target_non_malignant, f\"ct_synthetic_{filename}\")\n        shutil.copyfile(src, dst)\n\nprint(f\"‚úÖ Combined Real + Synthetic Benign images into: {target_non_malignant}\")\nprint(f\"üßæ Total: {len(os.listdir(target_non_malignant))} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:21.531531Z","iopub.execute_input":"2025-04-12T20:43:21.531807Z","iopub.status.idle":"2025-04-12T20:44:02.778059Z","shell.execute_reply.started":"2025-04-12T20:43:21.531780Z","shell.execute_reply":"2025-04-12T20:44:02.777454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Define paths ===\nroot_dataset = \"/kaggle/working/Combined_Lung_Dataset\"\nnormal_dir = os.path.join(root_dataset, \"Normal\")\nbenign_combined_dir = os.path.join(root_dataset, \"Non-Malignant\")  # this contains real + synthetic Benign\nmalignant_dir = os.path.join(root_dataset, \"Malignant\")\n\n# Final binary class folder\nfinal_dataset = \"/kaggle/working/Final_Binary_Dataset\"\nfinal_non_malignant = os.path.join(final_dataset, \"Non-Malignant\")\nfinal_malignant = os.path.join(final_dataset, \"Malignant\")\n\n# === Create directories ===\nos.makedirs(final_non_malignant, exist_ok=True)\nos.makedirs(final_malignant, exist_ok=True)\n\n# === Copy Normal images to Non-Malignant ===\nfor file in os.listdir(normal_dir):\n    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n        src = os.path.join(normal_dir, file)\n        dst = os.path.join(final_non_malignant, f\"normal_{file}\")\n        shutil.copyfile(src, dst)\n\n# === Copy Benign (real + synthetic) to Non-Malignant ===\nfor file in os.listdir(benign_combined_dir):\n    if file.lower().endswith('.png') or file.lower().endswith(('.jpg', '.jpeg')):\n        src = os.path.join(benign_combined_dir, file)\n        dst = os.path.join(final_non_malignant, file)\n        shutil.copyfile(src, dst)\n\n# === Copy Malignant images ===\nfor file in os.listdir(malignant_dir):\n    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n        src = os.path.join(malignant_dir, file)\n        dst = os.path.join(final_malignant, file)\n        shutil.copyfile(src, dst)\n\nprint(\"‚úÖ Final Binary Dataset structure created at /kaggle/working/Final_Binary_Dataset\")\nprint(f\"üìÅ Non-Malignant: {len(os.listdir(final_non_malignant))} images\")\nprint(f\"üìÅ Malignant: {len(os.listdir(final_malignant))} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:02.778722Z","iopub.execute_input":"2025-04-12T20:44:02.778955Z","iopub.status.idle":"2025-04-12T20:44:05.751293Z","shell.execute_reply.started":"2025-04-12T20:44:02.778930Z","shell.execute_reply":"2025-04-12T20:44:05.750463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\n\nimage_paths = []\nlabels = []\nlabel_dict = {}\ndata_dir = \"/kaggle/working/Final_Binary_Dataset\"\nfor idx, class_name in enumerate(os.listdir(data_dir)):\n    class_dir = os.path.join(data_dir, class_name)\n    if os.path.isdir(class_dir):\n        label_dict[class_name] = idx\n        for file_name in os.listdir(class_dir):\n            file_path = os.path.join(class_dir, file_name)\n            image_paths.append(file_path)\n            labels.append(idx)\n\nimage_paths = np.array(image_paths)\nlabels = np.array(labels)\n\nprint(\"‚úÖ Loaded image paths and labels.\")\nprint(\"üî¢ Label Mapping:\", label_dict)\nprint(\"üì¶ Total Images:\", len(image_paths))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:05.752016Z","iopub.execute_input":"2025-04-12T20:44:05.752210Z","iopub.status.idle":"2025-04-12T20:44:05.798440Z","shell.execute_reply.started":"2025-04-12T20:44:05.752195Z","shell.execute_reply":"2025-04-12T20:44:05.797719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = \"/kaggle/working/Final_Binary_Dataset\"\nnp.random.seed(42)\n\nimage_paths = []\nlabels = []\nsynthetic_paths = []\nsynthetic_labels = []\nlabel_dict = {}\n\n# === Load image paths and separate synthetic ===\nfor idx, class_name in enumerate(sorted(os.listdir(data_dir))):\n    class_dir = os.path.join(data_dir, class_name)\n    if os.path.isdir(class_dir):\n        label_dict[class_name] = idx\n        for file in os.listdir(class_dir):\n            full_path = os.path.join(class_dir, file)\n            if class_name == \"Non-Malignant\" and file.startswith(\"synthetic_\"):\n                synthetic_paths.append(full_path)\n                synthetic_labels.append(idx)\n            else:\n                image_paths.append(full_path)\n                labels.append(idx)\n\nimage_paths = np.array(image_paths)\nlabels = np.array(labels)\nsynthetic_paths = np.array(synthetic_paths)\nsynthetic_labels = np.array(synthetic_labels)\n\n# === Split real data into 70% train, 20% val, 10% test ===\nX_temp, X_test, y_temp, y_test = train_test_split(\n    image_paths, labels, test_size=0.1, stratify=labels, random_state=42)\n\nX_train_real, X_val, y_train_real, y_val = train_test_split(\n    X_temp, y_temp, test_size=2/9, stratify=y_temp, random_state=42)  # 2/9 of 90% ‚Üí ~20%\n\n# === Add synthetic data to training set only ===\nX_train = np.concatenate((X_train_real, synthetic_paths))\ny_train = np.concatenate((y_train_real, synthetic_labels))\n\n# === Done ===\nprint(\"‚úÖ 70/20/10 Split complete (synthetic used only in training).\")\nprint(f\"üì¶ Training Set: {len(X_train)} images (with synthetic)\")\nprint(f\"üß™ Validation Set: {len(X_val)} images (real only)\")\nprint(f\"üß´ Test Set: {len(X_test)} images (real only)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:05.799212Z","iopub.execute_input":"2025-04-12T20:44:05.799503Z","iopub.status.idle":"2025-04-12T20:44:05.878149Z","shell.execute_reply.started":"2025-04-12T20:44:05.799477Z","shell.execute_reply":"2025-04-12T20:44:05.877557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\ndef print_distribution(y, name=\"\"):\n    counts = Counter(y)\n    total = len(y)\n    print(f\"üìä {name} Set Distribution:\")\n    for label, count in sorted(counts.items()):\n        percentage = (count / total) * 100\n        print(f\"  Class {label} ‚Üí {count} samples ({percentage:.2f}%)\")\n    print(f\"  Total: {total} images\\n\")\n\n# Example:\nprint_distribution(y_train, \"Train\")\nprint_distribution(y_val, \"Validation\")\nprint_distribution(y_test, \"Test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:05.878839Z","iopub.execute_input":"2025-04-12T20:44:05.879108Z","iopub.status.idle":"2025-04-12T20:44:05.888681Z","shell.execute_reply.started":"2025-04-12T20:44:05.879088Z","shell.execute_reply":"2025-04-12T20:44:05.887972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nimage_size = (256, 256)\nmean = tf.convert_to_tensor([0.485, 0.456, 0.406], dtype=tf.float32)\nstd = tf.convert_to_tensor([0.229, 0.224, 0.225], dtype=tf.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:05.891081Z","iopub.execute_input":"2025-04-12T20:44:05.891270Z","iopub.status.idle":"2025-04-12T20:44:06.062709Z","shell.execute_reply.started":"2025-04-12T20:44:05.891256Z","shell.execute_reply":"2025-04-12T20:44:06.061409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef augment_image(image, label, filename):\n    # Apply base spatial transforms to all images\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    if tf.random.uniform([]) > 0.7:\n        k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n        image = tf.image.rot90(image, k)\n\n    # === Conditional Augmentations ===\n    # For histopathology images (H&E)\n    if tf.strings.regex_full_match(filename, \".*histo.*|.*real.*\"):\n        if tf.random.uniform([]) > 0.7:\n            image = tf.image.random_brightness(image, max_delta=0.08)\n        if tf.random.uniform([]) > 0.7:\n            image = tf.image.random_contrast(image, 0.9, 1.1)\n        if tf.random.uniform([]) > 0.7:\n            image = tf.image.random_saturation(image, 0.9, 1.1)\n        if tf.random.uniform([]) > 0.9:\n            image = tf.image.random_jpeg_quality(image, 80, 100)\n        if tf.random.uniform([]) > 0.7:\n            image = tf.image.central_crop(image, 0.9)\n            image = tf.image.resize(image, [256, 256])\n        if tf.random.uniform([]) > 0.7:\n            noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.01)\n            image = tf.clip_by_value(image + noise, 0.0, 1.0)\n\n    # For CT/synthetic images\n    elif tf.strings.regex_full_match(filename, \".*ct.*|.*synthetic.*\"):\n        # Less aggressive augmentations for CT\n        if tf.random.uniform([]) > 0.8:\n            image = tf.image.central_crop(image, 0.95)\n            image = tf.image.resize(image, [256, 256])\n        if tf.random.uniform([]) > 0.8:\n            # Slight noise for CT images\n            noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.002)\n            image = tf.clip_by_value(image + noise, 0.0, 1.0)\n        if tf.random.uniform([]) > 0.2:  # Small probability for brightness change for CT\n            image = tf.image.random_brightness(image, max_delta=0.02)  # Small brightness for CT scans\n\n    return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:06.063670Z","iopub.execute_input":"2025-04-12T20:44:06.063959Z","iopub.status.idle":"2025-04-12T20:44:06.075009Z","shell.execute_reply.started":"2025-04-12T20:44:06.063933Z","shell.execute_reply":"2025-04-12T20:44:06.074091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================= Preprocessing Function ================= #\ndef load_and_preprocess_image(image_path, label, augment=False):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, image_size)\n    print(\"Before normalization:\", tf.reduce_min(img), tf.reduce_max(img))\n    img = img / 255.0  # Normalize pixel values\n    print(\"After division by 255:\", tf.reduce_min(img), tf.reduce_max(img))\n    # Apply augmentation only if augment=True\n    if augment:\n        img, label = augment_image(img, label, filename)\n    print(\"After mean/std normalization:\", tf.reduce_min(img), tf.reduce_max(img))\n    img = (img - mean) / std  # Apply mean-std normalization\n    img = tf.cast(img, tf.float32)\n    return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:06.075805Z","iopub.execute_input":"2025-04-12T20:44:06.076054Z","iopub.status.idle":"2025-04-12T20:44:06.101312Z","shell.execute_reply.started":"2025-04-12T20:44:06.076031Z","shell.execute_reply":"2025-04-12T20:44:06.100529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================= Dataset Creation ================= #\ndef create_dataset(image_paths, labels, batch_size=32, shuffle=True, augment=False):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda x, y: load_and_preprocess_image(x, y, augment), num_parallel_calls=tf.data.AUTOTUNE)\n    if shuffle:\n        dataset = dataset.shuffle(len(image_paths))\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:06.102250Z","iopub.execute_input":"2025-04-12T20:44:06.102562Z","iopub.status.idle":"2025-04-12T20:44:06.125280Z","shell.execute_reply.started":"2025-04-12T20:44:06.102502Z","shell.execute_reply":"2025-04-12T20:44:06.124565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Create datasets\ntrain_dataset = create_dataset(X_train, y_train, augment=True)  # Augmentation only in training\nval_dataset = create_dataset(X_val, y_val, shuffle=False)\ntest_dataset = create_dataset(X_test, y_test, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:06.126028Z","iopub.execute_input":"2025-04-12T20:44:06.126227Z","iopub.status.idle":"2025-04-12T20:44:07.133140Z","shell.execute_reply.started":"2025-04-12T20:44:06.126213Z","shell.execute_reply":"2025-04-12T20:44:07.132527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Take batches and flatten to 50 individual images\nimages = []\nlabels = []\n\n# Collect 50 images\nfor batch_images, batch_labels in train_dataset:\n    for i in range(len(batch_images)):\n        images.append(batch_images[i])\n        labels.append(batch_labels[i])\n        if len(images) == 50:\n            break\n    if len(images) == 50:\n        break\n\n# Plot 50 images in a 10x5 grid\nplt.figure(figsize=(20, 10))  # Adjust size as needed\n\nfor i in range(50):\n    plt.subplot(5, 10, i + 1)\n    plt.imshow(tf.clip_by_value(images[i], 0.0, 1.0).numpy())\n    plt.title(f\"{int(labels[i])}\", fontsize=8)\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:07.133785Z","iopub.execute_input":"2025-04-12T20:44:07.133962Z","iopub.status.idle":"2025-04-12T20:44:45.923964Z","shell.execute_reply.started":"2025-04-12T20:44:07.133948Z","shell.execute_reply":"2025-04-12T20:44:45.922993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\nimport matplotlib.pyplot as plt\n\n# ‚úÖ Enable Mixed Precision for improved training performance\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy(\"mixed_float16\")\n\n# ================== Model Builder ==================#\ndef create_resnet50_model(input_shape=(256, 256, 3), num_classes=1, learning_rate=0.0001):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False  # Freeze initially\n\n    inputs = tf.keras.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-6))(x)\n    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n    model = models.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=SGD(learning_rate=learning_rate, momentum=0.9),  # SGD with momentum\n        loss='binary_crossentropy',  # Use binary cross entropy for binary classification\n        metrics=['accuracy']\n    )\n    return base_model, model\n\n# ================== Learning Rate Scheduler ==================#\ndef scheduler(epoch, lr):\n    if epoch < 5:\n        return lr\n    return lr * 0.5  # Reduce learning rate after the 5th epoch\n\ncallback_lr = LearningRateScheduler(scheduler)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"best_model_resnet50.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n\n# ================== Get the Model ==================#\nbase_model, model = create_resnet50_model(input_shape=(256, 256, 3))\n\n# ================== Initial Training (Frozen Base) ==================#\nhistory_lr_scheduler = model.fit(\n    train_dataset,  # Use the appropriate dataset\n    validation_data=val_dataset,\n    epochs=10,\n    callbacks=[callback_lr, early_stopping, model_checkpoint],\n    verbose=1\n)\n\n# ================== Fine-tuning ==================#\nbase_model.trainable = True\nfine_tune_at = 50  # Unfreeze layers from a certain point\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = False\n\nmodel.compile(\n    optimizer=SGD(learning_rate=1e-4, momentum=0.9),  # Use SGD with momentum during fine-tuning\n    loss='binary_crossentropy',  # Use binary cross entropy for binary classification\n    metrics=['accuracy']\n)\n\nhistory_fine = model.fit(\n    train_dataset,  # Use the appropriate dataset\n    validation_data=val_dataset,\n    epochs=10,  # Fine-tune for a few more epochs\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# ================== Evaluation ==================#\ntest_loss, test_acc = model.evaluate(test_dataset)\nprint(f\"\\n‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\nprint(f\"‚úÖ Test Loss: {test_loss:.4f}\")\n\n# ================== Plot Accuracy & Loss ==================#\nplt.plot(history_lr_scheduler.history['accuracy'], label='Train Accuracy')\nplt.plot(history_lr_scheduler.history['val_accuracy'], label='Val Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history_lr_scheduler.history['loss'], label='Train Loss')\nplt.plot(history_lr_scheduler.history['val_loss'], label='Val Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.show()\n\n# ================== Save Final Model ==================#\nmodel.save('/kaggle/working/resnet50_lung_model.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:44:45.925056Z","iopub.execute_input":"2025-04-12T20:44:45.925388Z","iopub.status.idle":"2025-04-12T21:13:11.019421Z","shell.execute_reply.started":"2025-04-12T20:44:45.925361Z","shell.execute_reply":"2025-04-12T21:13:11.018663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score\n\n# === Run Predictions ===\ny_true = []\ny_pred = []\ny_prob = []\n\nfor x_batch, y_batch in test_dataset:\n    probs = model.predict(x_batch).flatten()\n    preds = (probs > 0.5).astype(int)\n\n    y_true.extend(y_batch.numpy().astype(int))\n    y_pred.extend(preds)\n    y_prob.extend(probs)\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\ny_prob = np.array(y_prob)\n\n# === Metrics ===\naccuracy = np.mean(y_true == y_pred)\nf1 = f1_score(y_true, y_pred, average='macro')\nprecision = precision_score(y_true, y_pred, average='macro')\nrecall = recall_score(y_true, y_pred, average='macro')\nauroc = roc_auc_score(y_true, y_prob)\n\n# === Confusion Matrix ===\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Malignant\", \"Non-Malignant\"], yticklabels=[\"Malignant\", \"Non-Malignant\"])\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# === Print Results ===\nprint(\"‚úÖ Model Evaluation Metrics:\")\nprint(f\"Accuracy     : {accuracy * 100:.2f}%\")\nprint(f\"Macro F1     : {f1:.4f}\")\nprint(f\"Precision    : {precision:.4f}\")\nprint(f\"Recall       : {recall:.4f}\")\nprint(f\"AUROC        : {auroc:.4f}\")\n\n# === Classification Report ===\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=[\"Malignant\", \"Non-Malignant\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:13:11.020551Z","iopub.execute_input":"2025-04-12T21:13:11.020950Z","iopub.status.idle":"2025-04-12T21:13:30.478813Z","shell.execute_reply.started":"2025-04-12T21:13:11.020932Z","shell.execute_reply":"2025-04-12T21:13:30.478223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/resnet50_lung_model.zip /kaggle/working/resnet50_lung_model.h5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:13:30.479568Z","iopub.execute_input":"2025-04-12T21:13:30.480073Z","iopub.status.idle":"2025-04-12T21:13:40.961947Z","shell.execute_reply.started":"2025-04-12T21:13:30.480048Z","shell.execute_reply":"2025-04-12T21:13:40.961181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.savefig('/kaggle/working/confusion_matrix.png')  # Save confusion matrix as an image\n\n# === Save Results to a Text File ===\nwith open('/kaggle/working/model_evaluation.txt', 'w') as f:\n    f.write(\"‚úÖ Model Evaluation Metrics:\\n\")\n    f.write(f\"Accuracy     : {accuracy * 100:.2f}%\\n\")\n    f.write(f\"Macro F1     : {f1:.4f}\\n\")\n    f.write(f\"Precision    : {precision:.4f}\\n\")\n    f.write(f\"Recall       : {recall:.4f}\\n\")\n    f.write(f\"AUROC        : {auroc:.4f}\\n\")\n    \n    f.write(\"\\nClassification Report:\\n\")\n    f.write(classification_report(y_true, y_pred, target_names=[\"Malignant\", \"Non-Malignant\"]))\n\n# === Print Results ===\nprint(\"‚úÖ Model Evaluation Metrics:\")\nprint(f\"Accuracy     : {accuracy * 100:.2f}%\")\nprint(f\"Macro F1     : {f1:.4f}\")\nprint(f\"Precision    : {precision:.4f}\")\nprint(f\"Recall       : {recall:.4f}\")\nprint(f\"AUROC        : {auroc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:13:40.962999Z","iopub.execute_input":"2025-04-12T21:13:40.963286Z","iopub.status.idle":"2025-04-12T21:13:40.998342Z","shell.execute_reply.started":"2025-04-12T21:13:40.963253Z","shell.execute_reply":"2025-04-12T21:13:40.997660Z"}},"outputs":[],"execution_count":null}]}